[{"content":"负载均衡 负载均衡是一种技术，用于在多个服务器之间分配网络流量。它可以确保每个服务器都得到足够的流量，从而提高应用程序的性能和可靠性。在 Kubernetes 中，负载均衡是一个非常重要的概念，因为它可以确保应用程序在多个节点上运行时仍然可以正常工作。\n负载均衡的类型 在 Kubernetes 中，有两种类型的负载均衡：\n内部负载均衡：内部负载均衡是在 Kubernetes 集群内部运行的负载均衡器。它可以将流量从一个节点路由到另一个节点，从而确保应用程序在多个节点上运行时仍然可以正常工作。 外部负载均衡：外部负载均衡是在 Kubernetes 集群外部运行的负载均衡器。它可以将流量从 Internet 路由到 Kubernetes 集群内部的节点，从而确保应用程序在多个节点上运行时仍然可以正常工作。 负载均衡的实现 在 Kubernetes 中，有多种方法可以实现负载均衡。以下是一些常见的方法：\nService：Service 是 Kubernetes 中的一种资源类型，它可以将一组 Pod 路由到一个或多个节点上。Service 可以使用不同的负载均衡算法来实现负载均衡。 Ingress：Ingress 是 Kubernetes 中的一种资源类型，它可以将流量从 Internet 路由到 Kubernetes 集群内部的节点。Ingress 可以使用不同的负载均衡算法来实现负载均衡。 第三方负载均衡器：Kubernetes 还支持使用第三方负载均衡器来实现负载均衡。第三方负载均衡器可以将流量从 Internet 路由到 Kubernetes 集群内部的节点。 Service 中 kube-proxy 的负载均衡 Service 是 Kubernetes 中的一种资源类型，它可以将一组 Pod 路由到一个或多个节点上。Service 可以使用不同的负载均衡算法来实现负载均衡。\nkube-proxy 是 Kubernetes 中的一个组件，它可以将流量从 Service 路由到 Pod。kube-proxy 可以使用不同的负载均衡算法来实现负载均衡。 kube-proxy 支持以下负载均衡算法：\niptables 模式： (默认模式，性能相对较差) Kube-proxy 通过在 iptables 中添加规则来实现负载均衡。 当有请求到达时，iptables 规则会将请求随机转发到后端的 Pod。这种模式实现简单，但当 Service 数量较多时，iptables 规则会变得非常庞大，影响性能。 IPVS 模式： (高性能模式) Kube-proxy 使用 IPVS (IP Virtual Server) 来实现负载均衡。 IPVS 是一种内核级别的负载均衡器，性能比 iptables 更好。 IPVS 支持多种负载均衡算法，例如轮询、加权轮询、最小连接等。 Userspace 模式： (已弃用) Kube-proxy 在用户空间创建一个代理进程，监听 Service 的端口，并将请求转发到后端的 Pod。 这种模式性能最差，已经被弃用。 Ingress 中 Nginx 的负载均衡 概念： Ingress 是 K8s 中用于暴露 HTTP 和 HTTPS 服务的 API 对象。 它允许你使用域名或 URL 路径来路由流量到不同的 Service。 Ingress Controller： Ingress 本身只是一个 API 对象，需要 Ingress Controller 来实现 Ingress 的功能。 Ingress Controller 负责监听 API Server 中 Ingress 资源的变化，并根据 Ingress 的配置来配置负载均衡器（例如 Nginx、HAProxy）或云厂商的负载均衡器。 负载均衡： Ingress Controller 通常会使用 Nginx 或 HAProxy 等负载均衡器来实现 Ingress 的负载均衡。 这些负载均衡器可以根据域名、URL 路径、HTTP 头等信息来路由流量到不同的 Service。 Nginx 是一个开源的 Web 服务器和反向代理服务器。它可以作为 Ingress Controller 来实现 Ingress 的功能。\nNginx 支持以下负载均衡算法：\n轮询： (默认模式) Nginx 会将请求按顺序分配给后端的 Service。 加权轮询： Nginx 会根据 Service 的权重来分配请求。 权重越高的 Service，分配到的请求越多。 最小连接： Nginx 会将请求分配给连接数最少的 Service。 IP 哈希： Nginx 会根据客户端 IP 地址的哈希值来分配请求。 相同 IP 地址的请求会被分配到相同的 Service。 第三方负载均衡器 第三方负载均衡器是指在 Kubernetes 集群外部运行的负载均衡器。它们可以将流量从 Internet 路由到 Kubernetes 集群内部的节点。\n第三方负载均衡器可以使用不同的负载均衡算法来实现负载均衡。以下是一些常见的第三方负载均衡器：\nF5 BIG-IP： F5 BIG-IP 是一款高性能的负载均衡器，它可以将流量从 Internet 路由到 Kubernetes 集群内部的节点。 F5 BIG-IP 支持多种负载均衡算法，例如轮询、加权轮询、最小连接等。 HAProxy： HAProxy 是一款高性能的负载均衡器，它可以将流量从 Internet 路由到 Kubernetes 集群内部的节点。 HAProxy 支持多种负载均衡算法，例如轮询、加权轮询、最小连接等。 NGINX： NGINX 是一款高性能的负载均衡器，它可以将流量从 Internet 路由到 Kubernetes 集群内部的节点。 NGINX 支持多种负载均衡算法，例如轮询、加权轮询、最小连接等。 负载均衡的最佳实践 在 Kubernetes 中，有一些最佳实践可以帮助你实现负载均衡。以下是一些常见的最佳实践：\n使用 Service： Service 是 Kubernetes 中的一种资源类型，它可以将一组 Pod 路由到一个或多个节点上。使用 Service 可以确保应用程序在多个节点上运行时仍然可以正常工作。 使用 Ingress： Ingress 是 Kubernetes 中的一种资源类型，它可以将流量从 Internet 路由到 Kubernetes 集群内部的节点。使用 Ingress 可以确保应用程序在多个节点上运行时仍然可以正常工作。 使用第三方负载均衡器：使用第三方负载均衡器可以确保应用程序在多个节点上运行时仍然可以正常工作。 ","date":"2025-05-21T22:34:05+08:00","image":"https://www4.bing.com//th?id=OHR.SongyangTeaGarden_ZH-CN4763170909_UHD.jpg","permalink":"https://www.acgh.top/2025/05/load-balancing-in-kubernetes/","title":"Kubernetes 中的负载均衡"},{"content":"题目描述 给你一个整数数组 citations ，其中 citations[i] 表示研究者的第 i 篇论文被引用的次数，citations 已经按照 升序排列 。计算并返回该研究者的 h 指数。\nh 指数的定义：h 代表“高引用次数”（high citations），一名科研人员的 h 指数是指他（她）的 （n 篇论文中）至少 有 h 篇论文分别被引用了至少 h 次。\n请你设计并实现对数时间复杂度的算法解决此问题。\n解法 二分查找 二分查找 需要注意的是，二分查找的对象是 hindex，而不是 citations。\n如示例 1：\n输入：citations = [0,1,3,5,6]\n输出：3\n解释：给定数组表示研究者总共有 5 篇论文，每篇论文相应的被引用了 0, 1, 3, 5, 6 次。 由于研究者有3篇论文每篇 至少 被引用了 3 次，其余两篇论文每篇被引用 不多于 3 次，所以她的 h 指数是 3 。\nhindex = [0,1,2,3,4] ,hindex 才是二分查找的对象。\nn = citations.length = 5\n假设当 hindex[i] = 0 时，至少有 0 篇论文引用次数 \u0026gt;= 0，此时 citations[n - i] = 6 \u0026gt;=0, 所以 hindex = 0 是一个有效的答案。 假设当 hindex[i] = 1 时，至少有 1 篇论文引用次数 \u0026gt;= 1，此时 citations[n - i] = 5 \u0026gt;=1, 所以 hindex = 1 是一个有效的答案。 假设当 hindex[i] = 2 时，至少有 2 篇论文引用次数 \u0026gt;= 2，此时 citations[n - i] = 5 \u0026gt;=2, 所以 hindex = 2 是一个有效的答案。 假设当 hindex[i] = 3 时，至少有 3 篇论文引用次数 \u0026gt;= 3，此时 citations[n - i] = 3 \u0026gt;=3, 所以 hindex = 3 是一个有效的答案。 假设当 hindex[i] = 4 时，至少有 4 篇论文引用次数 \u0026gt;= 4，此时 citations[n - i] = 1 \u0026lt; 4, 所以 hindex = 4 不是一个有效的答案。 二分查找的目标是找到最大的 hindex，使得至少有 hindex 篇论文引用次数 \u0026gt;= hindex。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public int hIndex(int[] citations) { // 在区间 (left, right) 内询问 int n = citations.length; int left = 0; int right = n + 1; while (left + 1 \u0026lt; right) { // 区间不为空 // 循环不变量： // left 的回答一定为「是」 // right 的回答一定为「否」 int mid = left + (right - left) / 2; // 引用次数最多的 mid 篇论文，引用次数均 \u0026gt;= mid if (citations[n - mid] \u0026gt;= mid) { left = mid; // 询问范围缩小到 (mid, right) } else { right = mid; // 询问范围缩小到 (left, mid) } } // 根据循环不变量，left 现在是最大的回答为「是」的数 return left; } ","date":"2025-04-03T12:14:32+08:00","image":"https://www4.bing.com//th?id=OHR.MountHamilton_ZH-CN4280549129_UHD.jpg","permalink":"https://www.acgh.top/leetcode/2025/04/03/h-index-ii/","title":"275. H 指数 II"},{"content":"题目描述 给你一个按 非递减顺序 排列的数组 nums ，返回正整数数目和负整数数目中的最大值。\n换句话讲，如果 nums 中正整数的数目是 pos ，而负整数的数目是 neg ，返回 pos 和 neg二者中的最大值。 注意：0 既不是正整数也不是负整数。\n解法 遍历：时间复杂度 O(n)，空间复杂度 O(1) 二分查找：时间复杂度 O(logn)，空间复杂度 O(1) 遍历 遍历数组，统计负数数目 neg 和正数数目 pos。最后返回 max(neg,pos)。\n1 2 3 4 5 6 7 8 9 10 11 12 public int maximumCount(int[] nums) { int neg = 0; int pos = 0; for (int x : nums) { if (x \u0026lt; 0) { neg++; } else if (x \u0026gt; 0) { pos++; } } return Math.max(neg, pos); } 二分查找 由于数组是有序的，我们可以二分找到第一个 ≥0 的数的下标 i，那么下标在 [0,i−1] 中的数都小于 0，这恰好有 i 个。\n同样地，二分找到第一个 \u0026gt;0 的数的下标 j，那么下标在 [j,n−1] 中的数都大于 0，这有 n−j 个。\n所以通过二分查找第一个 ≥0 和第一个 \u0026gt;0 的位置，就可以用 O(logn) 的时间解决本题，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public int maximumCount(int[] nums) { int neg = lowerBound(nums, 0); // 找第一个大于等于0的元素 int pos = nums.length - lowerBound(nums, 1); return Math.max(neg, pos); } private int lowerBound(int[] nums, int target) { int left = -1, right = nums.length; while (left + 1 \u0026lt; right) { int mid = left + (right - left) / 2; if (nums[mid] == target) { right = mid; // 边界左缩 找到第一个等于target的元素 } else if (nums[mid] \u0026lt; target) { left = mid; } else if (nums[mid] \u0026gt; target) { right = mid; } } return right; } ","date":"2025-04-02T15:44:03+08:00","image":"https://www.acgh.top/leetcode/2025/04/02/maximum-count-of-positive-integer-and-negative-integer/2025-04-02-15-48-13_hu_30ecdd1eabd4af4d.png","permalink":"https://www.acgh.top/leetcode/2025/04/02/maximum-count-of-positive-integer-and-negative-integer/","title":"2529. 正整数和负整数的最大计数"},{"content":"题目描述 给你一个下标从 0 开始的数组 nums ，数组中有 n 个整数，另给你一个整数 k 。\n半径为 k 的子数组平均值 是指：nums 中一个以下标 i 为 中心 且 半径 为 k 的子数组中所有元素的平均值，即下标在 i - k 和 i + k 范围（含 i - k 和 i + k）内所有元素的平均值。如果在下标 i 前或后不足 k 个元素，那么 半径为 k 的子数组平均值 是 -1 。\n构建并返回一个长度为 n 的数组 avgs ，其中 avgs[i] 是以下标 i 为中心的子数组的 半径为 k 的子数组平均值 。\nx 个元素的 平均值 是 x 个元素相加之和除以 x ，此时使用截断式 整数除法 ，即需要去掉结果的小数部分。\n例如，四个元素 2、3、1 和 5 的平均值是 (2 + 3 + 1 + 5) / 4 = 11 / 4 = 2.75，截断后得到 2 。\n2090. 半径为 k 的子数组平均值\n2090. K Radius Subarray Averages\n解法 滑动窗口：时间复杂度 O(n)，空间复杂度 O(1) 滑动窗口 需要注意的是，在计算平均值时，需要使用 long 类型，否则会溢出。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public int[] getAverages(int[] nums, int k) { int left = 0, right = 0; long sum = 0L; // 子数组和 可能超出 int 范围 所以用 long 类型 int[] result = new int[nums.length]; Arrays.fill(result, -1); while (right \u0026lt; nums.length) { sum += nums[right]; right++; // 右指针右移 while (right - left \u0026gt;= 2 * k + 1) { // 子数组长度为 2k+1 即右指针 - 左指针 = 2k 进入子数组计算 result[right - k - 1] = (int) (sum / (2L * k + 1)); // 计算子数组最大平均数 即子数组和 / 子数组长度 sum -= nums[left]; left++; // 左指针右移 } } return result; } ","date":"2025-03-29T21:28:43+08:00","image":"https://www.acgh.top/leetcode/2025/03/29/k-radius-subarray-averages/2025-03-29-21-34-15_hu_62306c3db2da7da9.png","permalink":"https://www.acgh.top/leetcode/2025/03/29/k-radius-subarray-averages/","title":"2090. 半径为 k 的子数组平均值"},{"content":"题目描述 给定一个含有 n 个正整数的数组和一个正整数 target 。 找出该数组中满足其和 ≥ target 的长度最小的 连续子数组 [numsl, numsl+1, \u0026hellip;, numsr-1, numsr] ，并返回其长度。如果不存在符合条件的子数组，返回 0 。\n209. 长度最小的子数组\n209. Minimum Size Subarray Sum\n解法 滑动窗口：时间复杂度 O(n)，空间复杂度 O(1) 二分查找：时间复杂度 O(nlogn)，空间复杂度 O(n) 滑动窗口 滑动窗口需要两个指针，一个指向窗口的左边界，一个指向窗口的右边界。窗口的左边界和右边界都是从 0 开始，然后右边界不断右移，直到找到满足条件的子数组，然后左边界右移，直到不满足条件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public int minSubArrayLen(int target, int[] nums) { int left = 0, right = 0; int sum = 0; int res = Integer.MAX_VALUE; while (right \u0026lt; nums.length) { int num = nums[right]; right++; sum += num; // 收缩左边界 while (sum \u0026gt;= target) { res = Math.min(res, right - left); // 更新最小长度 要在左边界收缩之前更新最小长度 int num2 = nums[left]; // 要移除的元素 left++; // 左边界右移 sum -= num2; // 减去要移除的元素 } } return res == Integer.MAX_VALUE ? 0 : res; } 二分查找 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 public int minSubArrayLen(int target, int[] nums) { // Step 1: 初始化前缀和数组 int n = nums.length; long[] prefixSum = new long[n + 1]; // 使用 long 防止溢出 for (int i = 1; i \u0026lt;= n; i++) { prefixSum[i] = prefixSum[i - 1] + nums[i - 1]; } // Step 2: 遍历每个位置 i，用二分查找寻找满足条件的最小 j int minLength = Integer.MAX_VALUE; for (int i = 1; i \u0026lt;= n; i++) { // 我们需要找到 prefixSum[j] \u0026lt;= prefixSum[i] - target 的最大 j long requiredSum = prefixSum[i] - target; int j = binarySearch(prefixSum, requiredSum); // 如果找到了有效的 j，则更新最小长度 if (j \u0026gt;= 0) { minLength = Math.min(minLength, i - j); } } // Step 3: 返回结果 return minLength == Integer.MAX_VALUE ? 0 : minLength; } // 辅助函数：二分查找，找到最后一个小于等于 target 的索引 private int binarySearch(long[] prefixSum, long target) { int left = 0, right = prefixSum.length - 1; while (left \u0026lt;= right) { int mid = left + (right - left) / 2; if (prefixSum[mid] \u0026lt;= target) { left = mid + 1; // 继续向右找更大的值 } else { right = mid - 1; // 向左缩小范围 } } return right; // right 是最后一个小于等于 target 的索引 } ","date":"2025-03-27T23:00:32+08:00","image":"https://www.acgh.top/leetcode/2025/03/27/minimum-size-subarray-sum/2025-03-29-13-44-20_hu_729a8effa68ea579.png","permalink":"https://www.acgh.top/leetcode/2025/03/27/minimum-size-subarray-sum/","title":"209. 长度最小的子数组"},{"content":"NodePort NodePort 在 Kubernetes 里是一个广泛应用的服务暴露方式。 Kubernetes 中的 service 默认情况下都是使用的 ClusterIP 这种类型，这样的 service 会产生一个 ClusterIP ，这个 IP 只能在集群内部访问，要想让外部能够直接访问 service ，需要将 service type 修改为 nodePort 。\n如下就是一个 NodePort 的示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: nodeSelector: nginx: ok containers: - name: nginx image: nginx:stable command: [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] --- kind: Service apiVersion: v1 metadata: name: nginx-svc spec: type: NodePort ports: - port: 80 nodePort: 30001 selector: name: nginx 1 2 3 4 5 6 7 8 9 10 11 12 $ kube get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 176d \u0026lt;none\u0026gt; nginx-svc NodePort 10.110.21.50 \u0026lt;none\u0026gt; 80:30001/TCP 26s app=nginx $ kube get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES clubproxy-deployment-55c958bf67-cg424 1/1 Running 0 11h 10.244.0.56 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-c9fd6b54f-bdr8l 1/1 Running 0 18s 10.244.0.58 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ sudo netstat -lpn | grep 30001 tcp 0 0 0.0.0.0:30001 0.0.0.0:* LISTEN 15954/kube-proxy 如上配置，可以通过 {pod_ip}/index.html、{cluster_ip}/index.html、{node_ip}:30001/index.html 访问 nginx。node 节点上可以看到 kube-proxy 进程在监听 30001 端口，用于接收从主机网络进入的外部流量。最后，通过 iptables 规则将 {node_ip}:30001 映射到 {pod_ip}:容器端口。\nHostPort hostPort 是将容器端口与宿主节点上的端口建立映射关系，这样用户就可以通过宿主机的 IP 加上来访问 Pod 了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:stable command: [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] ports: - containerPort: 80 hostPort: 81 1 2 3 4 5 $ kube get pod -o wide|grep nginx nginx-deployment-ff549cb9d-5t4bp 1/1 Running 0 5m8s 10.244.0.40 k8s-master01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ sudo iptables -L -nv -t nat| grep \u0026#34;10.244.0.40:80\u0026#34; 0 0 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:81 to:10.244.0.40:80 如上配置，既可以通过 curl 10.244.0.40/index.html 访问 nginx ，也可以通过 curl {node_ip}:81/index.html访问nginx。可以看到 iptables 会添加一条 DNAT 规则，将宿主机的 81 端口映射到 Pod 的 80 端口。\nHostNetwork 这种网络模式 ，相当于 docker run --net=host，采用宿主机的网络命名空间。此时，Pod 的 ip 为 node 节点的 ip，Pod 的端口需要保持不与宿主机上的 port 端口发生冲突。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: hostNetwork: true containers: - name: nginx image: nginx:stable command: [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] 1 2 3 4 5 6 7 8 9 $ sudo ps aux| grep nginx root 20126 0.0 0.0 10640 3512 ? Ss 10:17 0:00 nginx: master process nginx -g daemon off; 101 20138 0.0 0.0 11044 1524 ? S 10:17 0:00 nginx: worker process 101 20139 0.0 0.0 11044 1524 ? S 10:17 0:00 nginx: worker process 101 20140 0.0 0.0 11044 1524 ? S 10:17 0:00 nginx: worker process 101 20141 0.0 0.0 11044 1524 ? S 10:17 0:00 nginx: worker process $ sudo netstat -lpn| grep \u0026#34;:80 \u0026#34; tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 20126/nginx: master 如上配置，可以直接通过 curl {node_ip}/index.html 访问 nginx ，并且由于 Pod 采用的是宿主机的网络命名空间，因此宿主机可以直接看到 nginx master 进程正在监听 80 端口。\n同时使用 HostPort 和 HostNetwork 如果在 deployment 中同时配置了 hostPort 和 hostNetwork ，那么 hostPort 会失效，只生效hostNetwork。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: apps/v1 kind: Deployment metadata: name: server-deployment labels: app: server spec: replicas: 3 selector: matchLabels: app: server template: metadata: labels: app: server spec: hostNetwork: true containers: - name: server image: techoc/server:alpine ports: - containerPort: 9697 hostPort: 9697 env: - name: PORT value: \u0026#34;9697\u0026#34; 1 2 3 4 5 6 7 8 9 10 $ k get po -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES server-deployment-648dc69468-srwzh 1/1 Running 0 48s 192.168.157.129 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; server-deployment-648dc69468-bxtjz 1/1 Running 0 48s 192.168.157.133 k8s-worker2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; server-deployment-648dc69468-p6bm2 1/1 Running 0 48s 192.168.157.132 k8s-worker1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ sudo netstat -lpn| grep \u0026#34;:9697\u0026#34; tcp6 0 0 :::9697 :::* LISTEN 17101/./server $ sudo iptables -L -nv -t nat| grep \u0026#34;:9697\u0026#34; 可以看到，yaml中填写了hostPort，和hostNetwork 。但是查看端口，发现端口被占用，并且端口被占用的进程是./server,查看 iptables 则并没有添加任何规则，\nHostPort 和 HostNetwork 的异同 相同点：\nhostPort 与 hostNetwork 本质上都是终端用户能访问到 pod 中的业务 访问的时候，只能用 pod 所在宿主机 IP + 容器端口或 hostport 端口 进行访问 不同点：\n网络地址空间不同。hostport 使用 CNI 分配的地址，hostNetwork 使用宿主机网络地址空间 hostport 宿主机不生成端口，hostNetwork 宿主机生成端口 hostport 通过 iptable 防火墙的 nat 表进行转发，hostNetwork 直接通过主机端口到容器中 定义的路径不同。deploy.spec.template.spec.containers.ports.hostPort 与 deploy.spec.template.spec.hostNetwork 优先级不同，hostNetwork 高于 hostPort ","date":"2024-03-20T10:20:15+08:00","image":"https://s.cn.bing.net/th?id=OHR.WhiteEyes_ZH-CN1130380430_1920x1080.jpg\u0026rf=LaDigue_1920x1080.jpg\u0026pid=hp","permalink":"https://www.acgh.top/k8s/2024/03/20/nodeport-hostport-hostnetwork/","title":"NodePort和HostPort与HostNetwork之间的区别"},{"content":" 作者: Amit Dsouza, Frederick Kautz, Kristin Martin, Abigail McCarthy, Natali Vlatko\n译者: Rui Yang\n快速预览：Kubernetes v1.30 的精彩新功能 新的一年，新的 Kubernetes 版本。我们已经进行了一半的发布周期，并且在 v1.30 中有许多有趣和令人兴奋的增强功能。从全新的 alpha 版本功能，到已有的功能升级到稳定版，再到期待已久的改进，这个版本对每个人都有值得关注的东西！\n在正式发布之前，让我们先来看一看这个周期中我们最为期待的增强功能的预览！\nKubernetes v1.30 的主要变更 为动态资源分配构建结构化参数 (KEP-4381) 在 Kubernetes v1.26 中作为 alpha 功能添加了动态资源分配。它定义了一种替代传统设备插件 API 的方法，用于请求访问第三方资源。根据设计，动态资源分配使用对于核心 Kubernetes 完全不透明的资源参数。这种方法对于集群自动缩放器（CA）或任何需要为一组 Pod（例如作业调度器）做出决策的更高级别控制器来说是个问题。它无法模拟随时间分配或释放资源请求的效果。只有第三方 DRA 驱动程序才有可用的信息来执行此操作。\n“动态资源分配的结构化参数”是对原始实现的扩展，通过构建一个框架来支持使这些声明参数不再那么不透明，从而解决了这个问题。驱动程序不再直接处理所有资源参数的语义，而是可以使用由 Kubernetes 预先定义的特定的“结构化模型”来管理资源并描述它们。这将允许了解这个“结构化模型”的组件在不将资源外包给某些第三方控制器的情况下做出关于这些资源的决策。例如，调度器可以快速分配资源而无需与动态资源分配驱动程序进行来回通信。这个版本的工作集中在定义必要的框架来启用不同的“结构化模型”以及实现“命名资源”模型上。这个模型允许列出各个资源实例，并且与传统的设备插件 API 相比，增加了通过属性单独选择这些实例的能力。\n节点内存交换支持 (KEP-2400) 在 Kubernetes v1.30 中，Linux 节点上的内存交换支持得到了重大变化，重点是改善系统稳定性。在先前的 Kubernetes 版本中，NodeSwap 特性门默认处于禁用状态，启用时，默认行为是使用 UnlimitedSwap。为了实现更好的稳定性，在 v1.30 中将删除 UnlimitedSwap 行为（可能会影响节点稳定性）。\n更新后的仍处于测试阶段的 Linux 节点上的交换支持将默认可用。然而，默认行为将是将节点设置为 NoSwap 模式（而不是 UnlimitedSwap）。在 NoSwap 模式下，kubelet 支持在交换空间处于活动状态的节点上运行，但 Pod 不使用任何页面文件。你仍然需要设置 --fail-swap-on=false 让 kubelet 在该节点上运行。然而，另一种模式的变化很大：LimitedSwap。在这种模式下，kubelet 实际上会使用该节点上的页面文件，并允许 Pod 将部分虚拟内存分页出去。容器（及其父 Pod）无法超出其内存限制访问交换，但系统仍然可以使用交换空间（如果可用的话）。\nKubernetes 的节点特别兴趣小组（SIG Node）还将根据来自最终用户、贡献者和更广泛的 Kubernetes 社区的反馈，更新文档，以帮助你理解如何使用修订后的实现。\n要获取有关 Kubernetes 中 Linux 节点交换支持的更多详细信息，请阅读以前的博客文章或节点交换文档。\n支持在 Pods 中使用用户命名空间 (KEP-127) 用户命名空间是一个仅适用于 Linux 的功能，它更好地隔离 Pods，以防止或减轻几个被评为高/严重的 CVE,包括CVE-2024-21626，于 2024 年 1 月发布。在 Kubernetes 1.30 中，对用户命名空间的支持正在迁移到 beta 版，并且现在支持具有和不具有卷、自定义 UID/GID 范围等功能的 Pods！\n结构化授权配置 (KEP-3221) 对结构化授权配置的支持正在迁移到 beta 版，并将默认启用。该功能使你能够创建具有多个 Webhook 的授权链，具有明确定义的参数，按特定顺序验证请求，并允许细粒度控制，例如在失败时明确拒绝。配置文件方法甚至允许你指定CEL规则，以在将请求分派给 Webhook 之前对其进行预过滤，帮助你防止不必要的调用。当配置文件被修改时，API 服务器还会自动重新加载授权链。\n你必须使用 --authorization-config 命令行参数指定授权配置的路径。如果你想继续使用命令行标志而不是配置文件，那么它们将继续像原样工作。要访问新的授权 Webhook 功能，如多个 Webhook、失败策略和预过滤规则，请切换到将选项放入 --authorization-config 文件中。从 Kubernetes 1.30 开始，配置文件格式处于 beta 级别，只需要指定 --authorization-config，因为默认情况下启用了功能门。在授权文档中提供了一个包含所有可能值的示例配置。更多详细信息，请阅读授权文档。\n基于容器资源的 Pod 自动缩放 (KEP-1610) 基于 ContainerResource 指标的水平 Pod 自动缩放将在 v1.30 中稳定发布。这种新的 HorizontalPodAutoscaler 行为允许你根据单个容器的资源使用情况配置自动缩放，而不是基于 Pod 的总体资源使用情况。欲了解更多详情，请参阅我们的先前文章，或阅读容器资源指标。\nCEL 用于准入控制 (KEP-3488) 在 Kubernetes 中集成通用表达式语言（CEL）用于准入控制引入了一种更动态和富有表现力的方式来评估准入请求。这个功能允许通过 Kubernetes API 直接定义和强制执行复杂、细粒度的策略，增强了安全性和治理能力，而不会影响性能或灵活性。\nCEL 的添加到 Kubernetes 准入控制赋予了集群管理员制定复杂规则的能力，这些规则可以评估 API 请求的内容与集群的期望状态和策略是否匹配，而无需求助于基于 Webhook 的访问控制器。这种控制水平对于维护集群操作的完整性、安全性和效率至关重要，使 Kubernetes 环境更加健壮，并能够适应各种用例和要求。有关使用 CEL 进行准入控制的更多信息，请参阅API 文档中的 ValidatingAdmissionPolicy 部分。\n我们希望你和我们一样对这个版本的发布感到兴奋。请密切关注几周后发布的官方发布博客，了解更多亮点！\n原文地址: https://kubernetes.io/blog/2024/03/12/kubernetes-1-30-upcoming-changes/\n官方译文: https://kubernetes.io/zh-cn/blog/2024/03/12/kubernetes-1-30-upcoming-changes/\n","date":"2024-03-16T16:00:05+08:00","image":"https://www.bing.com/th?id=OHR.BambooPanda_ZH-CN8455481760_1920x1080.jpg\u0026rf=LaDigue_1920x1080.jpg\u0026pid=hp","permalink":"https://www.acgh.top/k8s/2024/03/16/kubernetes-1-30-upcoming-changes/","title":"Kubernetes v1.30 初探"},{"content":"什么是容器启动命令 在创建 POD 时，我们可以为容器指定需要执行的启动命令以及参数。如果要设置命令，就填写在配置文件的 command 字段下，如果要设置命令的参数，就填写在配置文件的 args 字段下。 一旦 Pod 创建完成，该命令及其参数就无法再进行更改了。\n对于 Kubernetes 而言，配置文件中的 command 和 args 分别对应的是 Dockerfile 中的 ENTRYPOINT 和 CMD 命令。\n这意味着当你在 Kubernetes Pod 中指定 command 时，它会覆盖 Docker 镜像中定义的 ENTRYPOINT 。同样，当你指定 args 时，它会覆盖 Docker 镜像中定义的 CMD 。但会有一些特殊情况，比如只指定配置 command 的时候，只指定配置 args 的时候，下面会对这些情况进行详细说明\n在了解这几个特殊情况之前，我们先来了解下 Dockerfile 中的 ENTRYPOINT 和 CMD。\n对于 Dockerfile 中的 ENTRYPOINT 和 CMD 容器应用进程 = ENTRYPOINT + CMD。\nENTRYPOINT 定义了容器启动时运行的命令。它可以是一个可执行文件或者一个脚本，例如 nginx 容器的 Dockerfile下的 ENTRYPOINT 为 /docker-entrypoint.sh。如果在 Dockerfile 中没有显式配置 ENTRYPOINT，Docker 会使用一个默认的 ENTRYPOINT。这个默认的 ENTRYPOINT 是 /bin/sh -c。\n需要注意的是当一个 Dockerfile 中同时存在多条 ENTRYPOINT 时，只有最后一条ENTRYPOINT会生效。\nCMD 定义了容器启动时运行的命令的参数。CMD 提供了 ENTRYPOINT 的默认参数。如果没有提供 CMD，那么 ENTRYPOINT 需要是一个完整的命令。如果提供了 CMD，它可以作为 ENTRYPOINT 的参数。反之如果没有 ENTRYPOINT，则 CMD 需要是一个完整命令。\n使用的几种情况 替换而非追加：在 Kubernetes 中，当你设置 command 或 args 时，你是在替换 Docker 镜像中的 ENTRYPOINT 和 CMD，而不是在它们后面追加命令或参数。 只设置 command：当设置 command 时，Docker 镜像中的 ENTRYPOINT 将被替换，而 CMD 将被忽略。 只设置 args：当设置 args 时，Docker 镜像中的 CMD 将被替换，而 ENTRYPOINT 将被忽略。 k8s pod docker 最后生效的命令 command args entrypoint cmd 配置 配置 配置 配置 entrypoint+args 配置 不配置 配置 配置 command 不配置 配置 配置 配置 entrypoint+args 不配置 不配置 配置 配置 entrypoint+cmd 不配置 配置 不配置 配置 args 不配置 配置 配置 不配置 entrypoint+args 总而言之 Kubernetes 中的command对应于 Docker 的ENTRYPOINT，而args对应于 Docker 的CMD，并且在 Kubernetes Pod 定义中指定这些会覆盖 Docker 镜像中的相应设置。\n示例 有一个 例如 Bash 镜像 ，它有一个 ENTRYPOINT 和 CMD，如下所示：\n1 2 ENTRYPOINT [\u0026#34;docker-entrypoint.sh\u0026#34;] CMD [\u0026#34;bash\u0026#34;] 在这个 Dockerfile ，默认的ENTRYPOINT是docker-entrypoint.sh，而bash是传递给它的默认CMD。\n只设置 command，不设置 args Kubernetes Pod 的配置如下：\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: bash spec: containers: - name: python-container image: bash command: [\u0026#34;bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo hello\u0026#34;] # args: [\u0026#34;python3 --version\u0026#34;] restartPolicy: OnFailure 可以看到输出：\n1 2 # k logs bash hello 即所执行的命令为 bash -c echo hello bash(Kubernetes yaml 中的 command+ Dockerfile 中的 CMD)。\n只设置 args，不设置 command Kubernetes Pod 的配置如下：\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: bash spec: containers: - name: python-container image: bash # command: [\u0026#34;bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo hello\u0026#34;] args: [\u0026#34;python3 --version\u0026#34;] restartPolicy: OnFailure 可以看到输出：\n1 2 # k logs bash /usr/local/bin/docker-entrypoint.sh: line 11: exec: python3 --version: not found 即所执行的命令为 docker-entrypoint.sh python3 --version(Dockerfile 中的 ENTRYPOINT + Kubernetes yaml 中的 args)。\n设置 command 和 args Kubernetes Pod 的配置如下：\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: bash spec: containers: - name: python-container image: bash command: [\u0026#34;bash\u0026#34;, \u0026#34;-c\u0026#34;] args: [\u0026#34;echo hello\u0026#34;] restartPolicy: OnFailure 可以看到输出：\n1 2 # k logs bash hello 即所执行的命令为 bash -c echo hello(Kubernetes yaml 中的 command + Kubernetes yaml 中的 args)。\n不设置 command 和 args 即直接运行 Docker 镜像中的 ENTRYPOINT 和 CMD。\n","date":"2024-03-13T17:05:28+08:00","image":"https://cn.bing.com/th?id=OHR.AyutthayaTree_ZH-CN8075870220_1920x1080.jpg\u0026rf=LaDigue_1920x1080.jpg\u0026pid=hp","permalink":"https://www.acgh.top/k8s/2024/03/13/talk-about-define-command-argument-container/","title":"浅谈容器启动命令"},{"content":"Kubernetes 是一个开源的容器编排系统，用于自动化容器化应用的部署、扩展和运行。Kubernetes 集群中的每个节点都有一个 pod 数量限制，默认为 110。\n你可以通过修改 kubelet 的配置文件中的 maxPods 参数来修改 pod 数量限制。具体的配置文件可以查看。\n找到 kubelet 的配置文件路径。可以执行 systemctl status kubelet 命令查看： 1 2 3 4 5 6 7 8 9 10 11 12 [root@k8s-node01 kubelet]# systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /usr/lib/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since 一 2024-07-15 22:35:01 CST; 53min ago Docs: https://kubernetes.io/docs/ Main PID: 114544 (kubelet) Tasks: 11 Memory: 37.4M CGroup: /system.slice/kubelet.service └─114544 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/c... 可以看到，kubelet 的配置文件路径为 /var/lib/kubelet/config.yaml\n编辑 kubelet 的配置文件，将 maxPods 参数的值修改为你需要的 pod 数量限制。例如，将 maxPods 的值修改为 200： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.crt authorization: mode: Webhook webhook: cacheAuthorizedTTL: 0s cacheUnauthorizedTTL: 0s cgroupDriver: systemd clusterDNS: - 20.1.0.10 clusterDomain: cluster.local containerRuntimeEndpoint: \u0026#34;\u0026#34; cpuManagerReconcilePeriod: 0s evictionPressureTransitionPeriod: 0s fileCheckFrequency: 0s healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 0s imageMinimumGCAge: 0s kind: KubeletConfiguration logging: flushFrequency: 0 options: json: infoBufferSize: \u0026#34;0\u0026#34; verbosity: 0 memorySwap: {} nodeStatusReportFrequency: 0s nodeStatusUpdateFrequency: 0s rotateCertificates: true runtimeRequestTimeout: 0s shutdownGracePeriod: 0s shutdownGracePeriodCriticalPods: 0s staticPodPath: /etc/kubernetes/manifests streamingConnectionIdleTimeout: 0s syncFrequency: 0s volumeStatsAggPeriod: 0s maxPods: 200 重启 kubelet 服务，使修改生效 systemctl restart kubelet 验证修改: kubectl describe node k8s-node01 | grep maxPods ","date":"2024-03-13T17:05:28+08:00","image":"https://www.acgh.top/k8s/2024/03/13/how-to-increase-the-number-of-pods-limit-per-kubernetes-node/2024-07-15-22-51-03_hu_50bb2d16c82663df.png","permalink":"https://www.acgh.top/k8s/2024/03/13/how-to-increase-the-number-of-pods-limit-per-kubernetes-node/","title":"如何修改每个 Kubernetes 节点的最大 pod 数量限制"},{"content":" 作者: Sascha Grunert\n译者: Rui Yang\nSeccomp 代表安全计算模式，自 Linux 内核 2.6.12 版本起成为其特性。它可以用于沙箱化进程的特权，限制其从用户空间向内核发出的调用。Kubernetes 允许你自动应用加载到节点上的 seccomp 配置文件到你的 Pods 和容器。\n但是在 Kubernetes 中分发这些 seccomp 配置文件是一个重大的挑战，因为 JSON 文件必须在所有可能运行工作负载的节点上可用。像 Security Profiles 这样的项目通过在集群内运行守护进程来解决这个问题，这让我想知道哪些部分可以由 容器运行时 来完成。\n运行时通常从本地路径应用配置文件，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Pod metadata: name: pod spec: containers: - name: container image: nginx:1.25.3 securityContext: seccompProfile: type: Localhost localhostProfile: nginx-1.25.3.json 配置文件 nginx-1.25.3.json 必须位于 kubelet 的根目录中，并追加 seccomp 目录。这意味着配置文件在磁盘上的默认位置将是 /var/lib/kubelet/seccomp/nginx-1.25.3.json。如果配置文件不可用，那么容器创建时容器运行时会失败，如下所示：\n1 kubectl get pods 1 2 NAME READY STATUS RESTARTS AGE pod 0/1 CreateContainerError 0 38s 1 kubectl describe pod/pod | tail 1 2 3 4 5 6 7 8 9 10 Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 117s default-scheduler Successfully assigned default/pod to 127.0.0.1 Normal Pulling 117s kubelet Pulling image \u0026#34;nginx:1.25.3\u0026#34; Normal Pulled 111s kubelet Successfully pulled image \u0026#34;nginx:1.25.3\u0026#34; in 5.948s (5.948s including waiting) Warning Failed 7s (x10 over 111s) kubelet Error: setup seccomp: unable to load local profile \u0026#34;/var/lib/kubelet/seccomp/nginx-1.25.3.json\u0026#34;: open /var/lib/kubelet/seccomp/nginx-1.25.3.json: no such file or directory Normal Pulled 7s (x9 over 111s) kubelet Container image \u0026#34;nginx:1.25.3\u0026#34; already present on machine 需要手动分发Localhost配置文件这个主要障碍将导致许多终端用户退回到RuntimeDefault，甚至运行其工作负载为Unconfined(禁用 seccomp)。\nCRI-O 来解忧 Kubernetes 容器运行时 CRI-O 通过自定义注释提供各种功能。v1.30 版本添加了对一组新注释的支持，称为 seccomp-profile.kubernetes.cri-o.io/POD 和 seccomp-profile.kubernetes.cri-o.io/\u0026lt;CONTAINER\u0026gt;。这些注释允许你指定：\n一个特定容器的 seccomp 配置文件，当作为 seccomp-profile.kubernetes.cri-o.io/\u0026lt;CONTAINER\u0026gt; 使用时（例如：seccomp-profile.kubernetes.cri-o.io/webserver: 'registry.example/example/webserver:v1'） 一个 pod 中所有容器的 seccomp 配置文件，当不使用容器名称后缀但使用保留名称 POD 时（例如：seccomp-profile.kubernetes.cri-o.io/POD） 整个容器镜像的 seccomp 配置文件，如果镜像本身包含注释 seccomp-profile.kubernetes.cri-o.io/POD 或 seccomp-profile.kubernetes.cri-o.io/\u0026lt;CONTAINER\u0026gt;。 CRI-O 只有在运行时配置为允许的情况下才会遵守该注释，以及以 Unconfined 方式运行工作负载。所有其他的工作负载仍将 使用具有更高优先级的securityContext中的值\n仅仅使用注释并不能很好地解决配置文件的分发问题，但是它们可以被引用！例如，现在你可以像使用 OCI artifacts 一样指定 seccomp 配置文件：\n1 2 3 4 5 6 7 apiVersion: v1 kind: Pod metadata: name: pod annotations: seccomp-profile.kubernetes.cri-o.io/POD: quay.io/crio/seccomp:v2 spec: … 镜像 quay.io/crio/seccomp:v2 包含一个seccomp.json,该文件包含实际的配置文件内容。可以使用像 ORAS 或 Skopeo 这样的工具来检查镜像的内容：\n1 oras pull quay.io/crio/seccomp:v2 1 2 3 4 Downloading 92d8ebfa89aa seccomp.json Downloaded 92d8ebfa89aa seccomp.json Pulled [registry] quay.io/crio/seccomp:v2 Digest: sha256:f0205dac8a24394d9ddf4e48c7ac201ca7dcfea4c554f7ca27777a7f8c43ec1b 1 jq . seccomp.json | head 1 2 3 4 5 6 7 8 9 10 { \u0026#34;defaultAction\u0026#34;: \u0026#34;SCMP_ACT_ERRNO\u0026#34;, \u0026#34;defaultErrnoRet\u0026#34;: 38, \u0026#34;defaultErrno\u0026#34;: \u0026#34;ENOSYS\u0026#34;, \u0026#34;archMap\u0026#34;: [ { \u0026#34;architecture\u0026#34;: \u0026#34;SCMP_ARCH_X86_64\u0026#34;, \u0026#34;subArchitectures\u0026#34;: [ \u0026#34;SCMP_ARCH_X86\u0026#34;, \u0026#34;SCMP_ARCH_X32\u0026#34; 1 2 # Inspect the plain manifest of the image skopeo inspect --raw docker://quay.io/crio/seccomp:v2 | jq . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.cncf.seccomp-profile.config.v1+json\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:ca3d163bab055381827226140568f3bef7eaac187cebd76878e0b63e9e442356\u0026#34;, \u0026#34;size\u0026#34;: 3, }, \u0026#34;layers\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:92d8ebfa89aa6dd752c6443c27e412df1b568d62b4af129494d7364802b2d476\u0026#34;, \u0026#34;size\u0026#34;: 18853, \u0026#34;annotations\u0026#34;: { \u0026#34;org.opencontainers.image.title\u0026#34;: \u0026#34;seccomp.json\u0026#34; }, }, ], \u0026#34;annotations\u0026#34;: { \u0026#34;org.opencontainers.image.created\u0026#34;: \u0026#34;2024-02-26T09:03:30Z\u0026#34; }, } 镜像清单包含对特定所需配置媒体类型的引用（application/vnd. cncf.seccomp-profile.config.v1+json）和指向seccomp.json文件的单个层（application/vnd. oi.image.layer.v1.tar）。但是现在，让我们试一试这个新特征！\n使用注释来指定特定容器或整个 pod CRI-O 需要适当地配置才能使用该注释。可以通过将注释添加到运行时的 allowed_annotations 数组来实现。可以使用一个 drop-in 配置文件 /etc/crio/crio.conf.d/10-crun.conf 来实现：\n1 2 3 4 5 6 7 [crio.runtime] default_runtime = \u0026#34;crun\u0026#34; [crio.runtime.runtimes.crun] allowed_annotations = [ \u0026#34;seccomp-profile.kubernetes.cri-o.io\u0026#34;, ] 现在，让我们从最新的main提交中运行 CRI-O。可以通过从源代码构建，使用静态二进制包 或预发布软件包来实现这一点。\n为了演示，我在我的命令行中运行了crio二进制文件，使用local-up-cluster.sh来启动单节点 Kubernetes 集群。现在集群已经启动并运行，让我们尝试一个没有注释的 Pod，以Unconfined模式运行 seccomp：\n1 cat pod.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: pod spec: containers: - name: container image: nginx:1.25.3 securityContext: seccompProfile: type: Unconfined 1 kubectl apply -f pod.yaml 工作负载已启动并运行：\n1 kubectl get pods 1 2 NAME READY STATUS RESTARTS AGE pod 1/1 Running 0 15s 并且如果我使用crictl检查容器,则没有 seccomp 配置被应用：\n1 2 export CONTAINER_ID=$(sudo crictl ps --name container -q) sudo crictl inspect $CONTAINER_ID | jq .info.runtimeSpec.linux.seccomp 1 null 现在，让我们修改 pod，将配置文件quay.io/crio/seccomp:v2应用到容器：\n1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: Pod metadata: name: pod annotations: seccomp-profile.kubernetes.cri-o.io/container: quay.io/crio/seccomp:v2 spec: containers: - name: container image: nginx:1.25.3 我必须删除并重新创建 Pod，因为只有重新创建才会应用新的 Seccomp 配置文件：\n1 kubectl delete pod/pod 1 pod \u0026#34;pod\u0026#34; deleted 1 kubectl apply -f pod.yaml 1 pod/pod created CRI-O 日志将表明运行时拉取了该工件：\n1 2 3 4 WARN[…] Allowed annotations are specified for workload [seccomp-profile.kubernetes.cri-o.io] INFO[…] Found container specific seccomp profile annotation: seccomp-profile.kubernetes.cri-o.io/container=quay.io/crio/seccomp:v2 id=26ddcbe6-6efe-414a-88fd-b1ca91979e93 name=/runtime.v1.RuntimeService/CreateContainer INFO[…] Pulling OCI artifact from ref: quay.io/crio/seccomp:v2 id=26ddcbe6-6efe-414a-88fd-b1ca91979e93 name=/runtime.v1.RuntimeService/CreateContainer INFO[…] Retrieved OCI artifact seccomp profile of len: 18853 id=26ddcbe6-6efe-414a-88fd-b1ca91979e93 name=/runtime.v1.RuntimeService/CreateContainer 并且容器最终使用了这个配置：\n1 2 export CONTAINER_ID=$(sudo crictl ps --name container -q) sudo crictl inspect $CONTAINER_ID | jq .info.runtimeSpec.linux.seccomp | head 1 2 3 4 5 6 7 8 9 10 { \u0026#34;defaultAction\u0026#34;: \u0026#34;SCMP_ACT_ERRNO\u0026#34;, \u0026#34;defaultErrnoRet\u0026#34;: 38, \u0026#34;architectures\u0026#34;: [ \u0026#34;SCMP_ARCH_X86_64\u0026#34;, \u0026#34;SCMP_ARCH_X86\u0026#34;, \u0026#34;SCMP_ARCH_X32\u0026#34; ], \u0026#34;syscalls\u0026#34;: [ { 如果用户将/container后缀替换为保留名称/POD，则每个容器中的相同操作都会生效，例如：\n1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: Pod metadata: name: pod annotations: seccomp-profile.kubernetes.cri-o.io/POD: quay.io/crio/seccomp:v2 spec: containers: - name: container image: nginx:1.25.3 对容器镜像使用注释 在某些工作负载中将 seccomp 配置文件指定为 OCI 工件是一个很酷的功能，但大多数终端用户更愿意将 seccomp 配置文件与已发布的容器镜像关联起来。这可以通过使用容器镜像注释来实现；而不是应用于 Kubernetes Pod，该注释是应用于容器镜像本身的一些元数据。例如，Podman 可以在构建镜像时直接添加镜像注释：\n1 2 3 podman build \\ --annotation seccomp-profile.kubernetes.cri-o.io=quay.io/crio/seccomp:v2 \\ -t quay.io/crio/nginx-seccomp:v2 . 推送的镜像包含了这个注释：\n1 2 skopeo inspect --raw docker://quay.io/crio/nginx-seccomp:v2 | jq \u0026#39;.annotations.\u0026#34;seccomp-profile.kubernetes.cri-o.io\u0026#34;\u0026#39; 1 \u0026#34;quay.io/crio/seccomp:v2\u0026#34; 如果现在在一个 CRI-O 测试 Pod 定义中使用该镜像：\n1 2 3 4 5 6 7 8 9 apiVersion: v1 kind: Pod metadata: name: pod # no Pod annotations set spec: containers: - name: container image: quay.io/crio/nginx-seccomp:v2 然后，CRI-O 日志将显示镜像注释已被评估，并且配置文件已被应用：\n1 kubectl delete pod/pod 1 pod \u0026#34;pod\u0026#34; deleted 1 kubectl apply -f pod.yaml 1 pod/pod created 1 2 3 4 INFO[…] Found image specific seccomp profile annotation: seccomp-profile.kubernetes.cri-o.io=quay.io/crio/seccomp:v2 id=c1f22c59-e30e-4046-931d-a0c0fdc2c8b7 name=/runtime.v1.RuntimeService/CreateContainer INFO[…] Pulling OCI artifact from ref: quay.io/crio/seccomp:v2 id=c1f22c59-e30e-4046-931d-a0c0fdc2c8b7 name=/runtime.v1.RuntimeService/CreateContainer INFO[…] Retrieved OCI artifact seccomp profile of len: 18853 id=c1f22c59-e30e-4046-931d-a0c0fdc2c8b7 name=/runtime.v1.RuntimeService/CreateContainer INFO[…] Created container 116a316cd9a11fe861dd04c43b94f45046d1ff37e2ed05a4e4194fcaab29ee63: default/pod/container id=c1f22c59-e30e-4046-931d-a0c0fdc2c8b7 name=/runtime.v1.RuntimeService/CreateContainer 1 2 export CONTAINER_ID=$(sudo crictl ps --name container -q) sudo crictl inspect $CONTAINER_ID | jq .info.runtimeSpec.linux.seccomp | head 1 2 3 4 5 6 7 8 9 10 { \u0026#34;defaultAction\u0026#34;: \u0026#34;SCMP_ACT_ERRNO\u0026#34;, \u0026#34;defaultErrnoRet\u0026#34;: 38, \u0026#34;architectures\u0026#34;: [ \u0026#34;SCMP_ARCH_X86_64\u0026#34;, \u0026#34;SCMP_ARCH_X86\u0026#34;, \u0026#34;SCMP_ARCH_X32\u0026#34; ], \u0026#34;syscalls\u0026#34;: [ { 对于容器镜像，注释 seccomp-profile.kubernetes.cri-o.io 将被视为与 seccomp-profile.kubernetes.cri-o.io/POD 相同，并适用于整个 Pod。除此之外，当在镜像上使用容器特定的注释时，整个功能也会起作用，例如，如果一个容器的名称是 container1：\n1 2 skopeo inspect --raw docker://quay.io/crio/nginx-seccomp:v2-container | jq \u0026#39;.annotations.\u0026#34;seccomp-profile.kubernetes.cri-o.io/container1\u0026#34;\u0026#39; 1 \u0026#34;quay.io/crio/seccomp:v2\u0026#34; 这个整个功能的很酷之处在于，用户现在可以为特定的容器镜像创建 seccomp 配置文件，并将它们存储在同一个仓库中。将镜像与配置文件关联起来为在整个应用程序生命周期内维护它们提供了极大的灵活性。\n使用 ORAS 推送配置文件 当使用 ORAS 时，实际创建包含 seccomp 配置文件的 OCI 对象需要更多工作。我希望像 Podman 这样的工具将来会简化整个过程。当前，容器注册表需要是OCI 兼容的，这也适用于Quay.io。CRI-O 期望 seccomp 配置文件对象具有容器镜像媒体类型(application/vnd.cncf.seccomp-profile.config.v1+json)，而 ORAS 默认使用application/vnd.oci.empty.v1+json。为了实现所有这些，可以执行以下命令：\n1 2 3 4 echo \u0026#34;{}\u0026#34; \u0026gt; config.json oras push \\ --config config.json:application/vnd.cncf.seccomp-profile.config.v1+json \\ quay.io/crio/seccomp:v2 seccomp.json 生成的镜像包含了 CRI-O 所期望的mediaType。ORAS 将单个层seccomp.json推送到注册表中。配置文件的名称并不太重要。CRI-O 将选择第一个层，并检查其是否可以作为 seccomp 配置文件。\n将来的工作 CRI-O 内部管理的 OCI 资源就像普通的文件一样。这提供了将它们移动、删除或在 seccomp 配置文件之外提供其他数据的好处。这使得在 OCI 工件之上对 CRI-O 进行未来增强成为可能，同时也考虑将 seccomp 配置文件堆叠作为 OCI 工件中的多个层的一部分。对于 v1.30.x 版本,仅适用于 Unconfined 工作负载的这个限制,是 CRI-O 希望未来解决的问题。通过不牺牲安全性来简化整体用户体验似乎是容器工作负载中 seccomp 的成功未来的关键。\nCRI-O 的维护者将乐于听取有关这个新功能的任何反馈或建议！感谢您阅读这篇博客文章，欢迎通过 Kubernetes 的 Slack 频道#crio 或在 GitHub 存储库中创建问题与维护者联系。\n原文地址: https://kubernetes.io/blog/2024/03/07/cri-o-seccomp-oci-artifacts/\n","date":"2024-03-08T00:23:10+08:00","image":"https://s.cn.bing.net/th?id=OHR.IguazuFalls_ZH-CN4749837052_1920x1080.webp\u0026qlt=50","permalink":"https://www.acgh.top/k8s/2024/03/08/cri-o-seccomp-oci-artifacts/","title":"CRI-O：应用来自 OCI 注册中心的 seccomp 配置文件"},{"content":" 作者： Kevin Hannon (Red Hat)\n译者： Rui Yang\n运行或者操作 Kubernetes 集群的一个常见问题是磁盘空间不足。 在预分配节点时，你的目标应该是为容器镜像和正在运行的容器提供大量存储空间。\n容器运行时通常会写入到 /var目录下。 他可以位于单独的分区或者位于根文件系统上。 在默认情况下，CRI-O 将其容器和镜像写入 /var/lib/containers目录下，而 containerd 将其容器和镜像写入 /var/lib/containerd 目录下。\n在这篇博文中，我们希望引起您的注意，您可以配置容器运行时以将其内容与默认分区分开存储。 这使得配置 Kubernetes 变得更加灵活，并支持为容器存储添加更大的磁盘，同时保持默认文件系统不变。\n需要详细了解的是 Kubernetes 会写入到磁盘哪里以及写入哪些内容。\n理解 Kubernetes 磁盘的使用 Kubernetes 有持久化的数据和临时数据。kubelet 和本地 Kubernetes 特定的存储的根路径是可配置的，但是通常认为它位于 /var/lib/kubelet。在 Kubernetes 文档中，这有时被称为根或节点文件系统。该数据的大部分可以分为：\n临时存储 日志 容器运行时 这与大多数 POSIX 系统不同，因为根或者节点文件系统不是 /，而是 /var/lib/kubelet 所在的磁盘。\n临时存储 Pod 和容器可能需要临时或暂时的本地存储来进行操作。 临时存储的生命周期不会超过单个 pod 的生命周期，并且临时存储不能跨 pod 共享。\n日志 默认情况下，Kubernetes 会将每个运行容器的日志，作为文件存储在 /var/log 目录下。 这些日志是临时的，由 kubelet 监控，以确保它们在运行期间不会变得太大。\n你可以自定义每个节点的 日志轮转 设置，以管理这些日志的大小，以及配置日志传输（使用第三方解决方案）以避免依赖于节点本地存储。\n容器运行时 容器运行时有两个不同的存储区域用于容器和镜像。\n只读层: 镜像通常被标记为只读层，因为它们在容器运行时不会被修改。 只读层可以由多个层组成，然后组合成一个单独的只读层。 在容器的顶层上，有一个薄层提供容器的临时存储，如果容器在文件系统上写入，则其可以提供为临时存储。 可写层: 依赖于您的容器运行时，本地写入可能会使用基于层的写机制（例如，Linux 上的 overlayfs 或 Windows 上的 CimFS ）。 这被称为可写层。 本地写入也可以使用可写文件系统，该文件系统使用容器镜像的完整克隆进行初始化;这用于基于虚拟机管理程序的一些运行时。 容器运行时文件系统包含只读层和可写层。 在 Kubernetes 文档中，这被称为 imagefs。\n容器运行时配置 CRI-O CRI-O 使用 TOML 格式的存储配置文件，让你可以控制容器运行时存储持久化和临时数据。 CRI-O 使用 storage library。 一些 Linux 发行版提供了存储的手册页 (man 5 containers-storage.conf)。 存储的主要配置位于 /etc/containers/storage.conf ，用户可以控制临时数据的位置和根目录。 根目录是 CRI-O 存储持久数据的位置。\n1 2 3 4 5 6 7 [storage] # 默认存储驱动程序 driver = \u0026#34;overlay\u0026#34; # 临时存储位置 runroot = \u0026#34;/var/run/containers/storage\u0026#34; # 容器存储的主要读/写位置 graphroot = \u0026#34;/var/lib/containers/storage\u0026#34; graphroot 容器运行时的持久数据存储 如果启用 SELinux，则必须与 /var/lib/containers/storage 相匹配 runroot 临时访问容器的读/写访问 建议将此放在临时文件系统中 这里有一个快速的方法来重新标记你的 graphroot 目录，以匹配/var/lib/containers/storage:\n1 2 semanage fcontext -a -e /var/lib/containers/storage \u0026lt;YOUR-STORAGE-PATH\u0026gt; restorecon -R -v \u0026lt;YOUR-STORAGE-PATH\u0026gt; containerd containerd 使用 TOML 配置文件来控制持久数据和临时数据的存储位置。 默认的配置文件路径位于 /etc/containerd/config.toml。\ncontainerd 存储的相关字段是 root 和 state。\nroot 容器运行时元数据的根目录 默认位置为 /var/lib/containerd 如果您的操作系统需要，Root 也需要 SELinux 标签 state 容器运行时的临时数据 默认位置为 /run/containerd Kubernetes 节点压力驱逐 Kubernetes 将自动检测容器文件系统是否与节点文件系统分离。 当将文件系统分离时，Kubernetes 负责监控节点文件系统和容器运行时文件系统。 Kubernetes 文档将节点文件系统和容器运行时文件系统称为 nodefs 和 imagefs。 如果 nodefs 或 imagefs 中的任何一个磁盘空间不足，则整个节点被认为存在磁盘压力。 Kubernetes 首先通过删除未使用的容器和镜像来回收空间，然后将逐出 Pod。 在具有 nodefs 和 imagefs 的节点上，kubelet 将在 imagefs 上使用垃圾收集来清理未使用的容器镜像，并从 nodefs 中删除死亡的 Pod 及其容器。 如果只有一个 nodefs，则 Kubernetes 垃圾收集包括死亡的容器、死亡的 Pod 和未使用的镜像。\nKubernetes 提供了许多配置选项来确定磁盘是否已满。 kubelet 中的驱逐管理器有一些配置设置，可以让你控制相关的阈值。 对于文件系统，相关的度量是nodefs.available、nodefs.inodesfree、imagefs.available和imagefs.inodesfree。 如果没有专门用于容器运行时的磁盘，则会忽略 imagefs。\n用户可以使用现有的默认值有：\nmemory.available \u0026lt; 100MiB nodefs.available \u0026lt; 10% imagefs.available \u0026lt; 15% nodefs.inodesFree \u0026lt; 5% (Linux 节点) Kubernetes 允许您在 kubelet 配置文件中设置用户自定义值，具体位置为 EvictionHard 和 EvictionSoft 配置项。\nEvictionHard 定义限制；一旦超过这些限制，Pod 将立即驱逐，没有任何宽限期。 EvictionSoft 定义限制；一旦超过这些限制，Pod 将按照各信号所设置的宽限期后被。 如果你为 EvictionHard 指定了值，那么它将替代默认值。 这意味着在你的配置中设置所有的信号显得非常重要。\n举个例子，以下的 kubelet 配置可用于配置驱逐信号和宽限期选项。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration address: \u0026#34;192.168.0.8\u0026#34; port: 20250 serializeImagePulls: false evictionHard: memory.available: \u0026#34;100Mi\u0026#34; nodefs.available: \u0026#34;10%\u0026#34; nodefs.inodesFree: \u0026#34;5%\u0026#34; imagefs.available: \u0026#34;15%\u0026#34; imagefs.inodesFree: \u0026#34;5%\u0026#34; evictionSoft: memory.available: \u0026#34;100Mi\u0026#34; nodefs.available: \u0026#34;10%\u0026#34; nodefs.inodesFree: \u0026#34;5%\u0026#34; imagefs.available: \u0026#34;15%\u0026#34; imagefs.inodesFree: \u0026#34;5%\u0026#34; evictionSoftGracePeriod: memory.available: \u0026#34;1m30s\u0026#34; nodefs.available: \u0026#34;2m\u0026#34; nodefs.inodesFree: \u0026#34;2m\u0026#34; imagefs.available: \u0026#34;2m\u0026#34; imagefs.inodesFree: \u0026#34;2m\u0026#34; evictionMaxPodGracePeriod: 60s 问题 Kubernetes 项目建议你遵循默认驱逐设置或着设置所有驱逐字段。 你可以使用默认设置或指定你自己的 evictionHard 设置。 如果你漏掉一个信号，那么 Kubernetes 将不会监控该资源。 管理员或用户常见的一个配置错误是将新的文件系统挂载在 /var/lib/containers/storage 或 /var/lib/containerd 目录上。Kubernetes 会检测到一个单独的文件系统，因此如果你已经这样做，请确保检查 imagefs.inodesfree 和 imagefs.available 符合你的需要。\n另一个令人困惑的是，当为节点定义一个镜像文件系统时，临时存储报告不会改变。镜像文件系统(imagefs)被用于存储容器镜像层；如果一个容器向自己的根文件系统写入，那么这种本地写入不会计入到容器镜像的大小中。容器运行时用于存储本地修改的位置由运行时本身决定，通常情况下，这个位置就是镜像文件系统。 如果一个 Pod 中的容器在 emptyDir 文件系统上写入数据，那么它将使用 nodefs 文件系统中的空间。 kubelet 一直根据 nodefs 表示的文件系统报告临时存储容量和分配情况;当临时写入操作实际上是写到镜像文件系统时，这种差别可能会让人困惑\n后续工作 为了修复临时存储报告的相关限制和为容器运行时提供更多的配置选项，SIG Node 正在开发 KEP-4191。 在 KEP-4191 中，Kubernetes 将检测可写层和只读层（镜像）是否分离。 这可以让我们将所有临时存储，包括可写层，都放在同一个磁盘上，同时还可以为镜像分配一个单独的磁盘。\n参与其中 如果你有兴趣参与其中，你可以加入 Kubernetes Node 特性兴趣小组 (SIG)。\n如果你有兴趣分享反馈，你可以分享到我们的#sig-node Slack 频道。 如果你还没有加入该 Slack 工作空间，请访问 https://slack.k8s.io/ 获取邀请。\n特别感谢所有提供出色评审、分享宝贵见解或建议主题想法的贡献者。\nPeter Hunt Mrunal Patel Ryan Phillips Gaurav Singh 原文地址: https://kubernetes.io/blog/2024/01/23/kubernetes-separate-image-filesystem/\n官方译文: https://kubernetes.io/zh-cn/blog/2024/01/23/kubernetes-separate-image-filesystem/\n","date":"2024-03-06T00:08:00+08:00","image":"https://img.picgo.net/2024/03/05/_20240305223322f97c305c6e79f24e.jpeg","permalink":"https://www.acgh.top/k8s/2024/03/06/kubernetes-separate-image-filesystem/","title":"镜像文件系统：配置 Kubernetes 将容器存储在单独的文件系统上"},{"content":"作者：Arujjwal Negi\n译者：Rui Yang\n开发者使用 Kubernetes 服务最流行的方式之一是通过云提供商，但您有没有想过云提供商如何做到这一点？ Kubernetes 与各种云提供商集成的整个过程是如何发生的？ 为了回答这个问题，让我们关注一下 SIG 云提供商。\nSIG 云提供商致力于在 Kubernetes 和各种云提供商之间创建无缝集成。他们维持 Kubernetes 生态系统的公平性和开放性。通过制定清晰的标准和要求，以确保每个云提供商都能与 Kubernetes 良好协作。他们也负责配置集群组件以实现云提供商集成。\n在聚焦 SIG 系列的这篇博客中，Arujjwal Negi 采访了 Michael McCune (Red Hat)，又名 elmiko，SIG 云提供商的联合主席，让我们来 深入了解该小组的工作。\n导言 Arujjwal：让我们从了解你开始。你能给我们介绍一下你自己以及你是如何从事 Kubernetes 的吗？\nMichael：嗨，我是 Michael McCune，社区里的大多数人都叫我的名字 elmiko。我做软件开发人员已经有很长时间了(我刚开始的时候 Windows 3.1 很流行!)，我第一次接触 Kubernetes 是作为机器学习和数据科学应用程序的开发人员;当时我所在的团队正在创建教程和示例，以演示在 Kubernetes 上使用 Apache Spark 等技术。也就是说，我对分布式系统感兴趣已经很多年了，当有机会加入一个直接在 Kubernetes 上工作的团队时，我立刻就抓住了它!\n功能和工作 Arujjwal：您能否向我们介绍一下 SIG Cloud Provider 的业务及其运作方式？\nMichael：SIG Cloud Provider 的成立是确保 Kubernetes 为所有基础设施提供商提供一个中立的接入点。 迄今为止，我们最大的任务是将 in-tree 云控制器提取并迁移到 out-of-tree 组件。 SIG 会定期召开会议，讨论进展情况和即将到来的任务，并解决出现的问题和错误。 此外，我们还充当云提供商子项目（例如 cloud provider 框架、特定的云控制器实现和 Konnectivity proxy）的协调点。\nArujjwal：在阅读了项目的 README之后，我了解到 SIG Cloud Provider 致力于 Kubernetes 与 cloud provider 的集成。那么整个过程是怎样进行的呢？\nMichael：运行 Kubernetes 最常见的方法之一是将其部署到云环境（AWS、Azure、GCP 等）。 通常，云基础设施具有增强 Kubernetes 性能的功能，例如，通过为服务对象提供弹性负载平衡。 为了确保 Kubernetes 能够一致地使用特定于云的服务，Kubernetes 社区创建了云控制器来解决这些集成点。 云提供商可以使用 SIG 维护的框架或遵循 Kubernetes 代码和文档中定义的 API 指南来创建自己的控制器。 我想指出的一件事是，SIG Cloud Provider 不处理 Kubernetes 集群中节点的生命周期； 对于这些类型的主题，SIG Cluster Lifecycle 和 Cluster API 项目是更合适的场所。\n重要的子项目 Arujjwal：这个 SIG 中有很多子项目。你能重点介绍一些最重要的内容以及它们的作用吗？\nMichael：我认为当今两个最重要的子项目是 cloud provider 框架 和 extraction/migration project。 cloud provider 框架是一个通用库，可帮助基础设施集成商为其基础设施构建云控制器。 该项目通常是新人加入 SIG 的起点。 extraction/migration project 是另一个大子项目，也是该框架存在的很大一部分原因。 了解一点历史可能有助于进一步解释：很长一段时间以来，Kubernetes 需要与底层基础设施进行某种集成，不一定是为了添加功能，而是为了了解实例终止等云事件。 云提供商集成内置于 Kubernetes 代码树中，因此创建了术语“树内”，详见该文章。 社区认为在主 Kubernetes 源码树中维护特定于提供者的代码的活动是不可取的。 社区的决定激发了 extraction and migration project 的创建，以删除“树内”云控制器，转而使用“树外”组件。\n原文地址: https://kubernetes.io/blog/2024/03/01/sig-cloud-provider-spotlight-2024/\n","date":"2024-03-02T16:30:31+08:00","image":"https://www4.bing.com//th?id=OHR.LeapingSquirrel_ZH-CN9112090462_1920x1080.jpg","permalink":"https://www.acgh.top/k8s/2024/03/02/sig-cloud-provider-spotlight-2024/","title":"聚焦 SIG 云提供商"}]